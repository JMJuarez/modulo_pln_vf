# Gu√≠a de Tests de Estr√©s

## üìã Tabla de Contenidos

1. [Introducci√≥n](#introducci√≥n)
2. [Tipos de Tests Implementados](#tipos-de-tests-implementados)
3. [Herramientas Utilizadas](#herramientas-utilizadas)
4. [Ejecuci√≥n de Tests](#ejecuci√≥n-de-tests)
5. [Interpretaci√≥n de Resultados](#interpretaci√≥n-de-resultados)
6. [M√©tricas y Objetivos](#m√©tricas-y-objetivos)
7. [Troubleshooting](#troubleshooting)

---

## Introducci√≥n

Este proyecto implementa una **suite completa de tests de estr√©s** para validar el rendimiento, escalabilidad y robustez del m√≥dulo de PLN bajo diferentes condiciones de carga.

### ¬øPor qu√© tests de estr√©s?

- **Prevenir ca√≠das en producci√≥n**: Identificar l√≠mites antes de deployment
- **Optimizar rendimiento**: Encontrar cuellos de botella
- **Validar SLAs**: Asegurar que se cumplen objetivos de latencia/throughput
- **Detectar memory leaks**: Validar estabilidad en carga sostenida
- **Planear escalabilidad**: Determinar capacidad de usuarios concurrentes

---

## Tipos de Tests Implementados

### 1. **Concurrent Load Testing** üîÄ

**Archivo**: `test_stress_concurrent.py::TestConcurrentLoad`

Simula m√∫ltiples usuarios concurrentes haciendo requests simult√°neos.

| Test | Usuarios | Queries/Usuario | Objetivo |
|------|----------|-----------------|----------|
| `test_concurrent_10_users` | 10 | 10 | Latencia <200ms |
| `test_concurrent_50_users` | 50 | 5 | Success rate >90% |
| `test_concurrent_100_users_breaking_point` | 100 | 3 | Encontrar l√≠mite |

**Prop√≥sito**: Validar que el sistema maneja m√∫ltiples usuarios simult√°neos sin degradaci√≥n significativa.

**Ejecuci√≥n**:
```bash
pytest tests/performance/test_stress_concurrent.py::TestConcurrentLoad -v -s
```

**M√©tricas clave**:
- ‚úÖ Success rate (% de requests exitosos)
- ‚úÖ Latencia promedio, m√≠nima, m√°xima, P95
- ‚úÖ Throughput (queries/segundo)

---

### 2. **Spike Testing** ‚ö°

**Archivo**: `test_stress_concurrent.py::TestSpikeLoad`

Simula picos s√∫bitos de tr√°fico (0 ‚Üí N usuarios en 1 segundo).

| Test | Spike | Objetivo |
|------|-------|----------|
| `test_sudden_spike_0_to_20_users` | 0‚Üí20 | Success rate >80% |

**Prop√≥sito**: Validar que el sistema se adapta a picos s√∫bitos sin colapsar.

**Escenario real**: Black Friday, viral marketing, eventos especiales.

**Ejecuci√≥n**:
```bash
pytest tests/performance/test_stress_concurrent.py::TestSpikeLoad -v -s
```

---

### 3. **Soak Testing (Endurance)** ‚è±Ô∏è

**Archivo**: `test_stress_concurrent.py::TestSoakTesting`

Carga sostenida durante per√≠odos prolongados (5+ minutos).

| Test | Duraci√≥n | Usuarios | Objetivo |
|------|----------|----------|----------|
| `test_sustained_load_5_minutes` | 5 min | 5 | Detectar degradaci√≥n |

**Prop√≥sito**:
- Detectar **memory leaks**
- Validar que no hay **degradaci√≥n temporal**
- Asegurar estabilidad a largo plazo

**Ejecuci√≥n**:
```bash
pytest tests/performance/test_stress_concurrent.py::TestSoakTesting -v -s -m slow
```

**An√°lisis**: El test divide el tiempo en ventanas de 1 minuto y compara la latencia inicial vs final.

---

### 4. **Resource Exhaustion Testing** üíæ

**Archivo**: `test_stress_concurrent.py::TestResourceExhaustion`

Valida uso de recursos (memoria, CPU, file descriptors).

| Test | Prop√≥sito |
|------|-----------|
| `test_memory_stability_under_load` | Memory leaks |
| `test_error_recovery` | Recuperaci√≥n post-error |

**Ejecuci√≥n**:
```bash
pytest tests/performance/test_stress_concurrent.py::TestResourceExhaustion -v -s
```

**Validaciones**:
- ‚úÖ Incremento de memoria <20% despu√©s de 1000 queries
- ‚úÖ Sistema se recupera despu√©s de errores

---

### 5. **Gradual Degradation Testing** üìâ

**Archivo**: `test_stress_concurrent.py::TestGradualDegradation`

Curva de degradaci√≥n con carga incremental (1 ‚Üí 5 ‚Üí 10 ‚Üí 20 ‚Üí 50 usuarios).

**Prop√≥sito**: Entender c√≥mo el sistema degrada gradualmente al aumentar la carga.

**Ejecuci√≥n**:
```bash
pytest tests/performance/test_stress_concurrent.py::TestGradualDegradation -v -s
```

---

### 6. **Locust Load Testing** ü¶ó

**Archivo**: `locustfile.py`

Tests de carga realistas con simulaci√≥n de comportamiento de usuario.

#### **User Classes Disponibles**:

1. **ReadOnlyUser** (90% del tr√°fico)
   - B√∫squedas normales
   - B√∫squedas con typos
   - B√∫squedas de nombres
   - Health checks

2. **NormalUser** (10% del tr√°fico)
   - Exploraci√≥n de grupos
   - Deletreo directo
   - Info del sistema

3. **StressUser** (desactivado por defecto)
   - Queries muy r√°pidas (50ms entre requests)
   - Para encontrar l√≠mites absolutos

#### **Ejecuci√≥n con Locust**:

**Opci√≥n 1: Web UI (Recomendado)**
```bash
locust -f tests/performance/locustfile.py --host=http://localhost:8000
# Luego abre: http://localhost:8089
```

**Opci√≥n 2: Headless (sin UI)**
```bash
# Light load (10 usuarios, 2 minutos)
locust -f tests/performance/locustfile.py --host=http://localhost:8000 \
       --users 10 --spawn-rate 2 --run-time 2m --headless

# Moderate load (50 usuarios, 5 minutos)
locust -f tests/performance/locustfile.py --host=http://localhost:8000 \
       --users 50 --spawn-rate 5 --run-time 5m --headless

# Heavy load (100 usuarios, 5 minutos)
locust -f tests/performance/locustfile.py --host=http://localhost:8000 \
       --users 100 --spawn-rate 10 --run-time 5m --headless
```

**Opci√≥n 3: Con reportes**
```bash
locust -f tests/performance/locustfile.py --host=http://localhost:8000 \
       --users 50 --spawn-rate 5 --run-time 5m --headless \
       --csv=results --html=report.html
```

---

## Herramientas Utilizadas

### 1. **Pytest** + **ThreadPoolExecutor**
- Tests de concurrencia con control fino
- An√°lisis detallado de m√©tricas
- Integraci√≥n con CI/CD

### 2. **Locust**
- Simulaci√≥n realista de usuarios
- Web UI para monitoreo en tiempo real
- Reportes HTML y CSV
- Distribuci√≥n de carga

### 3. **psutil**
- Monitoreo de memoria
- Detecci√≥n de memory leaks

---

## Ejecuci√≥n de Tests

### **Opci√≥n 1: Script Automatizado** (Recomendado)

```bash
./run_stress_tests.sh
```

Men√∫ interactivo con opciones:
1. Quick Test (30s)
2. Light Load (10 usuarios, 2 min)
3. Moderate Load (50 usuarios, 5 min)
4. Heavy Load (100 usuarios, 5 min)
5. Stress Test (200 usuarios, 5 min)
6. Concurrent Tests (pytest)
7. Soak Test (10 min)
8. Full Suite (>30 min)
9. Custom Test

### **Opci√≥n 2: Pytest Directo**

```bash
# Todos los tests de performance
pytest tests/performance/ -v -s -m performance

# Solo tests r√°pidos
pytest tests/performance/ -v -s -m "performance and not slow"

# Solo concurrencia
pytest tests/performance/test_stress_concurrent.py::TestConcurrentLoad -v -s

# Solo soak testing
pytest tests/performance/test_stress_concurrent.py::TestSoakTesting -v -s
```

### **Opci√≥n 3: Locust Directo**

Ver secci√≥n anterior.

---

## Interpretaci√≥n de Resultados

### M√©tricas Clave

| M√©trica | Bueno | Aceptable | Malo |
|---------|-------|-----------|------|
| **Latencia promedio** | <100ms | <200ms | >200ms |
| **Latencia P95** | <200ms | <500ms | >500ms |
| **Success rate** | >95% | >90% | <90% |
| **Throughput** | >50 q/s | >30 q/s | <30 q/s |
| **Memory increase (1000q)** | <10% | <20% | >20% |

### An√°lisis de Locust Reports

**Archivo**: `stress_results/*_report.html`

#### **Secciones importantes**:

1. **Request Statistics**
   - Total requests
   - Failures (%)
   - Average response time
   - Min/Max response time

2. **Response Time Percentiles**
   - 50th percentile (mediana)
   - 95th percentile
   - 99th percentile

3. **Charts**
   - Total Requests per Second
   - Response Times (ms)
   - Number of Users

#### **Qu√© buscar**:

‚úÖ **Buenas se√±ales**:
- Response time estable a lo largo del test
- Failure rate <1%
- RPS (requests/s) constante
- Sin picos anormales de latencia

‚ö†Ô∏è **Se√±ales de alerta**:
- Response time creciente
- Failure rate >5%
- RPS decreciente con usuarios constantes
- Picos de latencia >1s

üö® **Se√±ales cr√≠ticas**:
- Failure rate >20%
- Response time >5s
- Sistema no responde
- Errores 500/502/503

---

## M√©tricas y Objetivos

### SLAs Definidos

| M√©trica | Objetivo | Cr√≠tico |
|---------|----------|---------|
| Latencia P50 | <100ms | >200ms |
| Latencia P95 | <200ms | >500ms |
| Latencia P99 | <500ms | >1s |
| Availability | >99.5% | <99% |
| Throughput | >50 q/s | <30 q/s |
| Max concurrent users | 100 | 50 |
| Memory growth rate | <1% per hour | >5% per hour |

### Alertas Configuradas

El script `run_stress_tests.sh` genera alertas cuando:
- ‚ùå Success rate <90%
- ‚ùå Latencia promedio >200ms
- ‚ùå P95 latency >500ms
- ‚ùå Memory increase >20%

---

## Troubleshooting

### Problema: "Connection refused" o "Connection timeout"

**Causa**: Servidor no est√° corriendo o no acepta conexiones.

**Soluci√≥n**:
```bash
# Verificar que el servidor est√° corriendo
curl http://localhost:8000/health

# Si no est√° corriendo:
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

---

### Problema: Tests fallan con "Too many open files"

**Causa**: L√≠mite de file descriptors alcanzado.

**Soluci√≥n**:
```bash
# Ver l√≠mite actual
ulimit -n

# Aumentar l√≠mite (temporal)
ulimit -n 4096

# Aumentar l√≠mite (permanente) - agregar a ~/.bashrc
echo "ulimit -n 4096" >> ~/.bashrc
```

---

### Problema: Latencias muy altas en tests locales

**Causa**: Modelo no est√° cargado en cache, CPU limitado.

**Soluci√≥n**:
1. Hacer warm-up antes de tests:
```bash
pytest tests/performance/test_benchmarks.py::TestInitializationPerformance::test_initialization_with_cache -v
```

2. Reducir n√∫mero de usuarios:
```bash
./run_stress_tests.sh
# Seleccionar opci√≥n 2 (Light Load) en vez de 3 o 4
```

---

### Problema: Memory leaks detectados

**Causa**: Objetos no se liberan correctamente.

**Diagn√≥stico**:
```bash
# Ejecutar con memory profiler
pytest tests/performance/test_stress_concurrent.py::TestResourceExhaustion::test_memory_stability_under_load -v -s

# Ver output detallado
```

**Soluci√≥n**: Revisar c√≥digo para:
- Cerrar conexiones expl√≠citamente
- Liberar recursos despu√©s de uso
- Evitar referencias circulares

---

### Problema: Tests de Locust no encuentran el archivo

**Causa**: Path incorrecto.

**Soluci√≥n**:
```bash
# Ejecutar desde el directorio ra√≠z del proyecto
cd /path/to/modulo_pln
locust -f tests/performance/locustfile.py --host=http://localhost:8000
```

---

### Problema: Results directory no existe

**Causa**: Script no cre√≥ el directorio.

**Soluci√≥n**:
```bash
mkdir -p stress_results
```

---

## Mejores Pr√°cticas

### 1. **Antes de ejecutar tests de estr√©s**:

‚úÖ Asegurar que el servidor est√° en modo producci√≥n (no debug)
‚úÖ Warm-up: Ejecutar algunas queries primero
‚úÖ Cerrar otras aplicaciones pesadas
‚úÖ Verificar que hay suficiente memoria/CPU disponible

### 2. **Durante los tests**:

‚úÖ Monitorear logs del servidor
‚úÖ Usar `htop` o similar para ver uso de recursos
‚úÖ No interferir con el sistema (no ejecutar otras tareas pesadas)

### 3. **Despu√©s de los tests**:

‚úÖ Revisar reportes generados
‚úÖ Comparar con tests anteriores (regresi√≥n?)
‚úÖ Documentar hallazgos
‚úÖ Crear tickets para issues encontrados

---

## Roadmap de Tests de Estr√©s

### ‚úÖ Implementado

- [x] Tests de concurrencia (10, 50, 100 usuarios)
- [x] Tests de spike (0‚Üí20 usuarios)
- [x] Soak testing (5 minutos)
- [x] Resource exhaustion testing
- [x] Locust load testing
- [x] Script automatizado de ejecuci√≥n
- [x] Reportes HTML y CSV

### üöß En Progreso

- [ ] Tests distribuidos con Locust (m√∫ltiples workers)
- [ ] Integraci√≥n con CI/CD (GitHub Actions)
- [ ] Dashboards en tiempo real (Grafana + Prometheus)

### üìã Futuro

- [ ] Chaos engineering (kill random services)
- [ ] Network latency simulation
- [ ] Tests de failover y recovery
- [ ] Load testing en staging environment
- [ ] Benchmarking vs competidores

---

## Recursos Adicionales

- **Locust Documentation**: https://docs.locust.io
- **Pytest Documentation**: https://docs.pytest.org
- **Load Testing Best Practices**: https://www.blazemeter.com/blog/performance-testing-best-practices

---

## Contacto y Soporte

Para preguntas o issues relacionados con tests de estr√©s:

1. Revisar esta gu√≠a
2. Revisar los comentarios en el c√≥digo
3. Crear issue en el repositorio

---

**√öltima actualizaci√≥n**: 2025-11-26
